{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SKU_Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e15f0da078b4d93b5e91d81112bb889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b10f685b691d434e9dc1c9eaad2bb94e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_775e42be26e844a8ab410f74e6dae516",
              "IPY_MODEL_81cb29a0a2924c8da93554f3072615cc",
              "IPY_MODEL_04c098bd10314c67994e71c5eaa25cbe"
            ]
          }
        },
        "b10f685b691d434e9dc1c9eaad2bb94e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "775e42be26e844a8ab410f74e6dae516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1acd56421efc43d0bd265a2b2fcf1281",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1958b70c0f54f1b8b9c58adcb497711"
          }
        },
        "81cb29a0a2924c8da93554f3072615cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d68aced551444f86a93eca1c7ab739d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3761b08185bd42769f0ae9559cf32193"
          }
        },
        "04c098bd10314c67994e71c5eaa25cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_377c566a6b11472f9c777666b0c45207",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50/50 [03:20&lt;00:00,  4.04s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6ce2df6c5cf4651a39458a5edfdc0c8"
          }
        },
        "1acd56421efc43d0bd265a2b2fcf1281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1958b70c0f54f1b8b9c58adcb497711": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d68aced551444f86a93eca1c7ab739d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3761b08185bd42769f0ae9559cf32193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "377c566a6b11472f9c777666b0c45207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6ce2df6c5cf4651a39458a5edfdc0c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5aae0b2d82ec4211b91894d18c58b25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c3117e2b4f294e98b16e5d5dd01c7251",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78c52b3bc27d492caa6cb21bca3dfa85",
              "IPY_MODEL_ad3f7e0e7e604a4ea9f2944a17f6c1b8",
              "IPY_MODEL_d191a2dc13af4092b86778c144baceee"
            ]
          }
        },
        "c3117e2b4f294e98b16e5d5dd01c7251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78c52b3bc27d492caa6cb21bca3dfa85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_45fb4512c2b44636a0b9e3cf2e07b2c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_876c136ab99340b2a1cb32686a89536d"
          }
        },
        "ad3f7e0e7e604a4ea9f2944a17f6c1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_753f700ffd704821b9a068991a115a15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1284,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1284,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9185e7d380304abc9b8c03537195414a"
          }
        },
        "d191a2dc13af4092b86778c144baceee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d5eb3594745e447ca4d3ac9e3995cfda",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1284/1284 [00:08&lt;00:00, 149.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4c40cd8feec4fc988a89aa4d0683bf1"
          }
        },
        "45fb4512c2b44636a0b9e3cf2e07b2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "876c136ab99340b2a1cb32686a89536d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "753f700ffd704821b9a068991a115a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9185e7d380304abc9b8c03537195414a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5eb3594745e447ca4d3ac9e3995cfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4c40cd8feec4fc988a89aa4d0683bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0331439800040c1999eb61de14a43c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3dc8349bdbf24f97b75b1147c74436ce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_546f686e6ea349abbe2b2fa6a7950850",
              "IPY_MODEL_7e3b771741cf48fea8c390feea1cd91a",
              "IPY_MODEL_53d61dd314f247dab1bc6d5ac2e556b6"
            ]
          }
        },
        "3dc8349bdbf24f97b75b1147c74436ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "546f686e6ea349abbe2b2fa6a7950850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a8d91d21447149ecaaeb54b522268372",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee9c7446178e4943afb506b5a33d6134"
          }
        },
        "7e3b771741cf48fea8c390feea1cd91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d0cc7fd619244628a657d0fe22e6dac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da8e20bcadfd48758fa678113b5787f8"
          }
        },
        "53d61dd314f247dab1bc6d5ac2e556b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_60a3a2796cee4c66a79ad10117871c39",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 25/25 [00:17&lt;00:00,  1.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f5aa9681fae411a954f9ea7f6965203"
          }
        },
        "a8d91d21447149ecaaeb54b522268372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee9c7446178e4943afb506b5a33d6134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d0cc7fd619244628a657d0fe22e6dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da8e20bcadfd48758fa678113b5787f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60a3a2796cee4c66a79ad10117871c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f5aa9681fae411a954f9ea7f6965203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67cd05a2a73d4e5db7886d945f6d14e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a526e91db314f1e9a32ef2cd8250652",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f0dd1f3cf5e54a76a837b7b72ce71874",
              "IPY_MODEL_8935360068f942cebd95baa49ccc3fd7",
              "IPY_MODEL_d279a37a88a04ff887fbff14069553ef"
            ]
          }
        },
        "7a526e91db314f1e9a32ef2cd8250652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0dd1f3cf5e54a76a837b7b72ce71874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8db82d1fe5d4970ae0df43a08a0f990",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_190cd3b5e19a429d9dedab69f2a9715e"
          }
        },
        "8935360068f942cebd95baa49ccc3fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fb0a46fa3ebf453c948e713bd92c79ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2980,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2980,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_589679cffc6a4368b08c76a6bca4a9e8"
          }
        },
        "d279a37a88a04ff887fbff14069553ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e08bee32eaf549459ae237fb886f417d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2980/2980 [00:07&lt;00:00, 397.26it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80f2e2ab0f4d4eecbe627a63bed85a62"
          }
        },
        "c8db82d1fe5d4970ae0df43a08a0f990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "190cd3b5e19a429d9dedab69f2a9715e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb0a46fa3ebf453c948e713bd92c79ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "589679cffc6a4368b08c76a6bca4a9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e08bee32eaf549459ae237fb886f417d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80f2e2ab0f4d4eecbe627a63bed85a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Open Folder"
      ],
      "metadata": {
        "id": "tenuSwriThuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "SAVEFOLDER = '/content/drive/MyDrive/data'\n",
        "\n",
        "MODELPATH = SAVEFOLDER\n",
        "MODELFILENAME = 'cnnmodel.pth'\n",
        "#downloaded = drive.CreateFile({'id':id}) \n",
        "#downloaded.GetContentFile('SKUdata.csv') \n",
        "DATAPATH = '/content/drive/MyDrive/data/Test6_REF.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCNB9RDqTW3J",
        "outputId": "a902c384-0c67-414a-b26d-f9035f1a0ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt8elt1LKRrR"
      },
      "source": [
        "## Pre-processing the data\n",
        "\n",
        "* <b>Description splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;We are interested in modeling SKU descriptions, rather than longer chunks of text such as paragraphs or documents. The data comes as one description per row, so we split each description at space characters. We also remove any multiple spacing.\n",
        "\n",
        "* <b>Full description markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each description must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow the model to generate descriptions of appropriate length.\n",
        "\n",
        "* <b>Unknown tokens:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown tokens in the test corpora, all tokens that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO61vsyWLlpv"
      },
      "source": [
        "START = \"<s>\"   # Start-of-sentence token\n",
        "END = \"</s>\"    # End-of-sentence-token\n",
        "UNK = \"<UNK>\"   # Unknown word token\n",
        "PUNC = {} # Punctuation that marks end of sentence, '.', '?', '!'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37nlPlfsLrVp"
      },
      "source": [
        "import torchtext\n",
        "import random\n",
        "import pandas\n",
        "from google.colab import drive\n",
        "\n",
        "def preprocess(data, vocab=None):\n",
        "    final_data = []\n",
        "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    for row in data:\n",
        "        if type(row) != str: continue # filter blanks\n",
        "        row = [x if x != '<unk>' else UNK for x in row.split()] # create tokens\n",
        "        if vocab is not None:\n",
        "            row = [x if x in vocab else UNK for x in row] # process UNK if Vocab present\n",
        "        if row == [] or row.count('=') >= 2: continue # if done processing\n",
        "        sen = []\n",
        "        prev_punct, prev_quot = False, False\n",
        "        for word in row:\n",
        "            if prev_quot: # Sentences can end in double quote\n",
        "                if word[0] not in lowercase: # If a new sent is beginning after double quote\n",
        "                    final_data.append(sen)\n",
        "                    sen = []\n",
        "                    prev_punct, prev_quot = False, False\n",
        "            if prev_punct:\n",
        "                if word == '\"':\n",
        "                    prev_punct, prev_quot = False, True\n",
        "                else:\n",
        "                    if word[0] not in lowercase: # If a new sent is beginning after PUNC\n",
        "                        final_data.append(sen)\n",
        "                        sen = []\n",
        "                        prev_punct, prev_quot = False, False\n",
        "            if word in PUNC: prev_punct = True\n",
        "            sen += [word.upper()]\n",
        "        # if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n",
        "        final_data.append(sen)\n",
        "    vocab_was_none = vocab is None # test whether to build vocab\n",
        "    if vocab is None:\n",
        "        vocab = set(UNK) # seed with UNK\n",
        "    for i in range(len(final_data)):\n",
        "        final_data[i] = [START] + final_data[i] + [END] # apply beginning and end markers\n",
        "        if vocab_was_none:\n",
        "            for word in final_data[i]:\n",
        "                vocab.add(word) \n",
        "    return final_data, vocab\n",
        "\n",
        "def getDataset(path):\n",
        "    df = []\n",
        "\n",
        "    filedata = pandas.read_csv(path, \n",
        "                         header = 0, \n",
        "                         names = ['Code', 'Name', 'PRODHA', 'dataset'], \n",
        "                         index_col='Code')\n",
        "\n",
        "    df.append(filedata[filedata['dataset'] == 'train'])\n",
        "    df.append(filedata[filedata['dataset'] == 'test'])\n",
        "\n",
        "    train_dataset, vocab = preprocess(df[0]['Name'])\n",
        "    test_dataset, _ = preprocess(df[1], vocab)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def getAuth():\n",
        "  # Code to read csv file into Colaboratory:\n",
        "\n",
        "  !pip install -U -q PyDrive\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  link = 'https://drive.google.com/open?id=1DPZZQ43w8brRhbEMolgLqOWKbZbE-IQu' # The shareable link\n",
        "\n",
        "  fluff, id = link.split('=')\n",
        "  print (id) # Verify that you have everything after '='\n",
        "  return None\n",
        "\n",
        "train_dataset, test_dataset = getDataset(DATAPATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUG_Oce2qkT2"
      },
      "source": [
        "Test the loaded data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRcvl3rZqkrj",
        "outputId": "32ee32d5-91ef-4d7d-b1d3-ada49b21804f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    for x in random.sample(train_dataset, 10):\n",
        "        print (x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '430-37', 'CN1QT', 'PHTHALO', 'BLUE', 'FUL-BASE', '</s>']\n",
            "['<s>', '51910033053', 'CN1LT', '2K', 'HS', 'HARDENER', 'NORMAL', '</s>']\n",
            "['<s>', 'CH6115', 'CN1LT', 'CHALLENGER', 'OCEAN', 'BLUE', '</s>']\n",
            "['<s>', '1058S', 'BO50GR', 'GOLD', 'PEARL', '</s>']\n",
            "['<s>', 'WB05', 'CN1L', 'CROMAX', 'PRO', 'JET', 'BLACK', '</s>']\n",
            "['<s>', '02017159', 'CN0.5LT', 'STBLU', 'MIX159', 'DK', 'YEL', '</s>']\n",
            "['<s>', '40', 'CN1QT', 'LMB', '1:1', 'DTM', 'EPOXY', 'PRIMER', '</s>']\n",
            "['<s>', '02017120', 'CN0.5LT', 'STBLU', 'MIX120', 'BLU', 'PRL', '</s>']\n",
            "['<s>', 'XP53', 'CN1PT', 'RED', 'ORANGE', '</s>']\n",
            "['<s>', '29100003', 'CN1LT', 'PS', '2K', 'TRANSP', 'BLUE', 'MIDCOAT', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yuJQKc2qxZ_"
      },
      "source": [
        "## The LanguageModel Class\n",
        "\n",
        "Below are 4 types of language models: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model. \n",
        "\n",
        "* <b>`__init__(self, trainCorpus)`</b>: This trains the language model on `trainCorpus`, calculating relative frequency estimates according to the type of model.\n",
        "\n",
        "* <b>`generateSentence(self)`</b>: Returns a product description that is generated by the language model. Of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a token in the vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>).  <TT>&lt;s&gt;</TT> starts each description (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to the language model's distribution. Note that the number of tokens <TT>n</TT> is not fixed; instead, the description is finished as soon as the  stop token is generated <TT>&lt;&sol;s&gt;</TT>.\n",
        "\n",
        "* <b>`getSentenceLogProbability(self, sentence)`</b>:  Returns the <em> logarithm of the probability</em> of <TT>sentence</TT>, since we do the calcs in safer/quicker log space to avoid underflow, which is again a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>. This uses the base-<em>e</em> logarithm. \n",
        "\n",
        "* <b>`getCorpusPerplexity(self, testCorpus)`</b>:  Returns model perplexity (normalized inverse log probability) of `testCorpus` according to the model. For a corpus $W$ with $N$ tokens and a bigram model, Jurafsky and Martin (2020) defines perplexity as follows: \n",
        "\n",
        "$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n",
        "\n",
        "Instead of multiplying probabilities, we add the logarithms of the probabilities and exponentiate the result:\n",
        "\n",
        "$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ug7GnnsYQd"
      },
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class LanguageModel(object):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train the model (i.e. estimate the model's underlying probability\n",
        "        distribution from the training corpus.)\n",
        "        '''\n",
        "        return\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate a sentence by drawing words according to the model's probability distribution.\n",
        "        Note: Think about how to set the length of the sentence in a principled way.\n",
        "        '''\n",
        "        raise NotImplementedError(\"Implement generateSentence in each subclass.\")\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the log probability of the sentence provided. \n",
        "        '''\n",
        "        raise NotImplementedError(\"Implement getSentenceProbability in each subclass.\")\n",
        "        \n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexity of the corpus provided.\n",
        "        '''\n",
        "        raise NotImplementedError(\"Implement getCorpusPerplexity in each subclass.\")\n",
        "\n",
        "    def printSentences(self, n):\n",
        "        '''\n",
        "        Prints n sentences generated by the model.\n",
        "        '''\n",
        "\n",
        "        for i in range(n):\n",
        "            sent = self.generateSentence()\n",
        "            prob = self.getSentenceLogProbability(sent)\n",
        "            print('Log Probability:', prob , '\\tSentence:',sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmBSZeftyT43"
      },
      "source": [
        "## Unigram Model\n",
        "\n",
        "The <b>unsmoothed unigram</b> model. The probability distribution of a token is given by $\\hat P(w)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ChcH6jydrY",
        "outputId": "56868706-1b45-405f-ad66-46999714bb83"
      },
      "source": [
        "from collections import Counter\n",
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        self.flatCorp = []\n",
        "\n",
        "        # Count all uni-grams including UNK but not <s>, and allowing starting caps\n",
        "        self.c = self.getCount(trainCorpus)\n",
        "        \n",
        "        # Create the wheel of fortune\n",
        "        self.totalcount = sum(self.c.values())\n",
        "        self.wheel = dict.fromkeys(range(1, self.totalcount+1))\n",
        "        place = 1 \n",
        "        for word in self.c.keys():\n",
        "          for x in range(1, self.c[word] + 1):\n",
        "            self.wheel[place] = word\n",
        "            place += 1\n",
        "\n",
        "        # Calc log probs\n",
        "        self.logprobs = self.c\n",
        "        for word, count in self.c.items():\n",
        "          self.logprobs[word] = math.log(count / self.totalcount, math.e)           \n",
        "\n",
        "    def generateSentence(self):\n",
        "        # generate random number up to the wordcount to lookup token\n",
        "        # starting with one element, could be slow\n",
        "        sent = [START]\n",
        "        newword = '0'\n",
        "        while newword != END:\n",
        "          newword = self.generateWord()\n",
        "          sent.append(newword)\n",
        "        return sent\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        # skip <s>\n",
        "        total = [self.logprobs[word] if word in self.logprobs else self.logprobs[UNK] for word in sentence if word != START]\n",
        "        # N = len(sentence) - 1\n",
        "        return sum(total)\n",
        "        \n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        # iterate over testcorpus and get LM probs (no OOV possible in Wiki2)\n",
        "        # START not allowed (but START is needed in bigram...just not in N)\n",
        "        if testCorpus is not None:\n",
        "          self.flatCorpforprobs = [word for sentence in testCorpus for word in sentence] # get list of words\n",
        "          self.flatCorpforN = [word for sentence in testCorpus for word in sentence if word not in {START}] # get list of words\n",
        "          N = len(self.flatCorpforN) # the length of the total number of tokens\n",
        "          logprobs = [self.logprobs[word] for word in self.flatCorpforprobs if word in self.c.keys()]\n",
        "          return math.exp((-1/N) * sum(logprobs))\n",
        "        else:\n",
        "          return\n",
        "\n",
        "    def generateWord(self):\n",
        "        roll = random.randrange(1, self.totalcount+1)\n",
        "        return self.wheel[roll]\n",
        "\n",
        "    def getCount(self, corpus):\n",
        "        c = Counter()\n",
        "        for sent in corpus:\n",
        "          for word in sent:\n",
        "            c[word] += 1\n",
        "        del c['<s>']\n",
        "        return c\n",
        "###\n",
        "#twosent = train_dataset[1115:1117]\n",
        "test = UnigramModel(train_dataset)\n",
        "testsent = ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
        "#test.logprobs['</s>']\n",
        "test.getCorpusPerplexity(test_dataset)\n",
        "a = [count for words in test.c for count in words if count == 0]\n",
        "a\n",
        "#type(test.c)\n",
        "#test.getSentenceLogProbability(testsent)\n",
        "#test = UnigramModel(train_dataset)\n",
        "#test.printSentences(1)\n",
        "#test.getCorpusPerplexity(test_dataset)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 463
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JGONUjzyy-n"
      },
      "source": [
        "Here is a testing function that uses very simple training & test \n",
        "corpora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNy92nMTy-MZ",
        "outputId": "d96b2fc5-2d86-4683-dea3-60d10bdbeb48"
      },
      "source": [
        "def testModel(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "\n",
        "    #\tRead in the test corpus\n",
        "    train_corpus = [\"By the Late Classic , a network of <unk> ( <unk> ) linked various parts of the city , running for several kilometres through its urban core .\",\n",
        "    \"Few people realize how difficult it was to create Sonic 's graphics engine , which allowed for the incredible rate of speed the game 's known for .\"]\n",
        "    test_corpus = [\"Classic parts of the game allowed for <unk> incredible city .\", \n",
        "                   \"Few <unk> realize the difficult network , which linked the game to Sonic .\"]\n",
        "    train_corpus, _ = preprocess(train_corpus)\n",
        "    test_corpus, _ = preprocess(test_corpus)\n",
        "    sentence = preprocess([\"Sonic was difficult .\"])[0][0]\n",
        "\n",
        "    # These are the correct answers (don't change them!)\n",
        "    if model_type == \"unigram\":\n",
        "       senprobs = [-18.9159206916, -106.714608418, -107.8132207067, -43.0623556464, -54.9560026056]\n",
        "       trainPerp, testPerp = 40.3970060507, 37.7244929883\n",
        "       model = UnigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-unigram\":\n",
        "       senprobs = [-18.8969788221, -107.3856946234, -108.078841804, -43.7800012747, -55.3816031464]\n",
        "       trainPerp, testPerp = 41.0547195671, 39.356140682\n",
        "       model = SmoothedUnigramModel(train_corpus)\n",
        "    elif model_type == \"bigram\":\n",
        "       senprobs = [-float('inf'), -10.3450917073, -9.2464794186, -float('inf'), -float('inf')]\n",
        "       trainPerp, testPerp = 1.4018400696, float('inf')\n",
        "       model = BigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-bigram\":\n",
        "       senprobs = [-16.1336514995, -84.9068097328, -84.6431458887, -39.3603940053, -51.4605809045]\n",
        "       trainPerp, testPerp = 18.6021115212, 28.8970586024\n",
        "       model = SmoothedBigramModelAD(train_corpus)       \n",
        "\n",
        "    print(\"--- TEST: generateSentence() ---\")\n",
        "    modelSen = model.generateSentence()\n",
        "    senTestPassed = isinstance(modelSen, list) and len(modelSen) > 1 and isinstance(modelSen[0], str)\n",
        "    if senTestPassed:\n",
        "        print (\"Test generateSentence() passed!\")\n",
        "    else:\n",
        "        print (\"Test generateSentence() failed; did not return a list of strings...\")\n",
        "\n",
        "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
        "    sentences = [sentence, *train_corpus, *test_corpus]\n",
        "    failed = 0\n",
        "    for i in range(len(sentences)):\n",
        "        sen, correct_prob = sentences[i], senprobs[i]\n",
        "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
        "        print(\"Correct log prob.:\", correct_prob, '\\tModel log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
        "        if prob != correct_prob: failed+=1\n",
        "\n",
        "    if not failed:\n",
        "        print (\"Test getSentenceProbability(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
        "\n",
        "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
        "    train_perp = round(model.getCorpusPerplexity(train_corpus), 10)\n",
        "    test_perp = round(model.getCorpusPerplexity(test_corpus), 10)\n",
        "\n",
        "    print(\"Correct train perp.:\", trainPerp, '\\tModel train perp.:', train_perp, '\\t', 'PASSED' if trainPerp == train_perp else 'FAILED')\n",
        "    print(\"Correct test perp.:\", testPerp, '\\tModel test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
        "    train_passed, test_passed = train_perp == trainPerp, test_perp == testPerp\n",
        "    if train_passed and test_passed:\n",
        "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getCorpusPerplexity(...) failed on\", \"the training corpus and the testing corpus...\" if not train_passed and not test_passed else \"the testing corpus...\" if not test_passed else \"the training corpus...\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    testModel('unigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -18.9159206916 \tModel log prob.: -18.9159206916 \t PASSED \t ['<s>', 'SONIC', 'WAS', 'DIFFICULT', '.', '</s>']\n",
            "Correct log prob.: -106.714608418 \tModel log prob.: -106.714608418 \t PASSED \t ['<s>', 'BY', 'THE', 'LATE', 'CLASSIC', ',', 'A', 'NETWORK', 'OF', '<UNK>', '(', '<UNK>', ')', 'LINKED', 'VARIOUS', 'PARTS', 'OF', 'THE', 'CITY', ',', 'RUNNING', 'FOR', 'SEVERAL', 'KILOMETRES', 'THROUGH', 'ITS', 'URBAN', 'CORE', '.', '</s>']\n",
            "Correct log prob.: -107.8132207067 \tModel log prob.: -107.8132207067 \t PASSED \t ['<s>', 'FEW', 'PEOPLE', 'REALIZE', 'HOW', 'DIFFICULT', 'IT', 'WAS', 'TO', 'CREATE', 'SONIC', \"'S\", 'GRAPHICS', 'ENGINE', ',', 'WHICH', 'ALLOWED', 'FOR', 'THE', 'INCREDIBLE', 'RATE', 'OF', 'SPEED', 'THE', 'GAME', \"'S\", 'KNOWN', 'FOR', '.', '</s>']\n",
            "Correct log prob.: -43.0623556464 \tModel log prob.: -43.0623556464 \t PASSED \t ['<s>', 'CLASSIC', 'PARTS', 'OF', 'THE', 'GAME', 'ALLOWED', 'FOR', '<UNK>', 'INCREDIBLE', 'CITY', '.', '</s>']\n",
            "Correct log prob.: -54.9560026056 \tModel log prob.: -54.9560026056 \t PASSED \t ['<s>', 'FEW', '<UNK>', 'REALIZE', 'THE', 'DIFFICULT', 'NETWORK', ',', 'WHICH', 'LINKED', 'THE', 'GAME', 'TO', 'SONIC', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 40.3970060507 \tModel train perp.: 40.3970060507 \t PASSED\n",
            "Correct test perp.: 37.7244929883 \tModel test perp.: 37.7244929883 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW0YO4aSzVfh"
      },
      "source": [
        "Now, we can train the model on the full corpus, and evaluate it on the held-out test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dP03ceHz7oY",
        "outputId": "d6900ce2-a3e1-4c86-fb4c-e3183317b213"
      },
      "source": [
        "def runModel(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "    # Read the corpora\n",
        "    if model_type == 'unigram':\n",
        "        model = UnigramModel(train_dataset)\n",
        "    elif model_type == 'bigram':\n",
        "        model = BigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-unigram':\n",
        "        model = SmoothedUnigramModel(train_dataset)\n",
        "    else:\n",
        "        model = SmoothedBigramModelAD(train_dataset)\n",
        "\n",
        "    print(\"--------- 5 sentences from your model ---------\")\n",
        "    model.printSentences(5)\n",
        "\n",
        "    print (\"\\n--------- Corpus Perplexities ---------\")\n",
        "    print (\"Training Set:\", model.getCorpusPerplexity(train_dataset))\n",
        "    print (\"Testing Set:\", model.getCorpusPerplexity(test_dataset))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    runModel('unigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -87.59261245364787 \tSentence: ['<s>', 'RED', '02018151', 'ADDITIVE', 'METALUX', 'LV', 'CN1GA', 'CH5003', '1K', 'CN1QT', 'WHITE', 'BLACK', 'CN0.5LT', '410A', 'STBL', 'CN1GA', '</s>']\n",
            "Log Probability: -58.24251061914554 \tSentence: ['<s>', '-', 'FILLER', '02016252', 'NCN1L', 'BO4Z', 'DUX', 'HS', 'PRL', '</s>']\n",
            "Log Probability: -12.289536106928919 \tSentence: ['<s>', 'WHITE', 'PRL', '</s>']\n",
            "Log Probability: -1.8922777104005823 \tSentence: ['<s>', '</s>']\n",
            "Log Probability: -1.8922777104005823 \tSentence: ['<s>', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 362.9152094957221\n",
            "Testing Set: 2.575745109575581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odjuRdN91u35"
      },
      "source": [
        "## Smoothed Unigram Model\n",
        "\n",
        "A <b>unigram</b> model with <b>Laplace (add-one) smoothing</b>. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n",
        "\n",
        "In order to smooth the model, the number of words in the corpus, $N$, and the number of word types, $S$ is used. The distinction between these is meaningful: $N$ indicates the number of word instances, where $S$ refers to the size of our vocabulary. For example, the sentence <em>the cat saw the dog</em> has four word types (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>dog</em>), but five word tokens (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>the</em>, <em>dog</em>). The token <em>the</em> appears twice in the sentence, but they share the same type <em>the</em>.\n",
        "\n",
        "If $c(w)$ is the frequency of $w$ in the training data, $P_L(w)$ is:\n",
        "\n",
        "$$P_L(w)=\\frac{c(w)+1}{N+S}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONvaBTGx1uRu"
      },
      "source": [
        "class SmoothedUnigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        self.flatCorp = []\n",
        "\n",
        "        # Count all uni-grams including UNK but not <s>, and allowing starting caps\n",
        "        self.c = self.getCount(trainCorpus)\n",
        "        \n",
        "        # Create the wheel of fortune\n",
        "        self.totalcount = sum(self.c.values())\n",
        "        self.wheel = dict.fromkeys(range(1, self.totalcount+1))\n",
        "        place = 0 \n",
        "        for word in self.c.keys():\n",
        "          for x in range(1, self.c[word] + 1):\n",
        "            self.wheel[place] = word\n",
        "            place += 1\n",
        "\n",
        "        # Calc log probs\n",
        "        self.logprobs = self.c\n",
        "        for word, count in self.c.items():\n",
        "          self.logprobs[word] = math.log(count / self.totalcount, math.e)           \n",
        "\n",
        "    def generateSentence(self):\n",
        "        # generate random number up to the wordcount to lookup token\n",
        "        # starting with one element, could be slow\n",
        "        sent = [START]\n",
        "        newword = '0'\n",
        "        while newword != END:\n",
        "          newword = self.generateWord()\n",
        "          sent.append(newword)\n",
        "        return sent\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        # skip <s>\n",
        "        total = [self.logprobs[word] if word in self.logprobs else self.logprobs[UNK] for word in sentence if word != START]\n",
        "        # N = len(sentence) - 1\n",
        "        return sum(total)\n",
        "        \n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        # iterate over testcorpus and get LM probs (no OOV possible in Wiki2)\n",
        "        # START not allowed (but START is needed in bigram...just not in N)\n",
        "        if testCorpus is not None:\n",
        "          self.flatCorpforprobs = [word for sentence in testCorpus for word in sentence] # get list of words\n",
        "          self.flatCorpforN = [word for sentence in testCorpus for word in sentence if word not in {START}] # get list of words\n",
        "          N = len(self.flatCorpforN) # the length of the total number of tokens\n",
        "          logprobs = [self.logprobs[word] for word in self.flatCorpforprobs if word in self.c.keys()]\n",
        "          return math.exp((-1/N) * sum(logprobs))\n",
        "        else:\n",
        "          return\n",
        "\n",
        "    def generateWord(self):\n",
        "        roll = random.randrange(1, self.totalcount+1)\n",
        "        return self.wheel[roll]\n",
        "\n",
        "    def getCount(self, corpus):\n",
        "        c = Counter()\n",
        "        for sent in corpus:\n",
        "          for word in sent:\n",
        "            c[word] += 1\n",
        "        del c['<s>']\n",
        "        # Do SMOOTHING\n",
        "        for word in c:\n",
        "          c[word] += 1\n",
        "        return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoEM1B112E5o",
        "outputId": "882ccd75-5860-4bed-e8a2-c2a86cdbab8b"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    testModel('smoothed-unigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -18.8969788221 \tModel log prob.: -18.8969788221 \t PASSED \t ['<s>', 'SONIC', 'WAS', 'DIFFICULT', '.', '</s>']\n",
            "Correct log prob.: -107.3856946234 \tModel log prob.: -107.3856946234 \t PASSED \t ['<s>', 'BY', 'THE', 'LATE', 'CLASSIC', ',', 'A', 'NETWORK', 'OF', '<UNK>', '(', '<UNK>', ')', 'LINKED', 'VARIOUS', 'PARTS', 'OF', 'THE', 'CITY', ',', 'RUNNING', 'FOR', 'SEVERAL', 'KILOMETRES', 'THROUGH', 'ITS', 'URBAN', 'CORE', '.', '</s>']\n",
            "Correct log prob.: -108.078841804 \tModel log prob.: -108.078841804 \t PASSED \t ['<s>', 'FEW', 'PEOPLE', 'REALIZE', 'HOW', 'DIFFICULT', 'IT', 'WAS', 'TO', 'CREATE', 'SONIC', \"'S\", 'GRAPHICS', 'ENGINE', ',', 'WHICH', 'ALLOWED', 'FOR', 'THE', 'INCREDIBLE', 'RATE', 'OF', 'SPEED', 'THE', 'GAME', \"'S\", 'KNOWN', 'FOR', '.', '</s>']\n",
            "Correct log prob.: -43.7800012747 \tModel log prob.: -43.7800012747 \t PASSED \t ['<s>', 'CLASSIC', 'PARTS', 'OF', 'THE', 'GAME', 'ALLOWED', 'FOR', '<UNK>', 'INCREDIBLE', 'CITY', '.', '</s>']\n",
            "Correct log prob.: -55.3816031464 \tModel log prob.: -55.3816031464 \t PASSED \t ['<s>', 'FEW', '<UNK>', 'REALIZE', 'THE', 'DIFFICULT', 'NETWORK', ',', 'WHICH', 'LINKED', 'THE', 'GAME', 'TO', 'SONIC', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 41.0547195671 \tModel train perp.: 41.0547195671 \t PASSED\n",
            "Correct test perp.: 39.356140682 \tModel test perp.: 39.356140682 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npURZ0ma2K8m"
      },
      "source": [
        "## Bigram Model\n",
        "\n",
        "An <b>unsmoothed bigram</b> model follows, where the probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xPbzYIU2UAj"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        self.bigrams = [] # list of bigrams as appeared\n",
        "        self.unigrams = []\n",
        "        self.vocab = {} # vocab words set\n",
        "        self.c = {} # count of bigrams, no dupes\n",
        "        self.c1 = {}\n",
        "        self.wheels = defaultdict(dict)\n",
        "        self.logprobs = {}\n",
        "\n",
        "        self.getVocab(trainCorpus)\n",
        "        #self.N = sum(self.c.values()) # total count of n-grams in corpus\n",
        "        self.getAllProbs()\n",
        "\n",
        "    def generateSentence(self):\n",
        "        # Start with START, get next word, repeat till END\n",
        "        sent = [START]\n",
        "        newWord = ''\n",
        "        while newWord != END:\n",
        "          newWord = self.generateBigram(sent[-1])\n",
        "          sent.append(newWord)\n",
        "        return sent        \n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        # prob firstword * (2nd | 1st) * (3rd | 2nd) * (END | nth)\n",
        "        # lookup the stem count, lookup the bigram count, take log of quotient, sum repeat\n",
        "        bigramProb = 0.0\n",
        "        sentbigrams = [tuple(sentence[i:i+2]) for i in range(len(sentence) - (2-1))]\n",
        "        for bigram in sentbigrams:\n",
        "          stem = bigram[0] # first word of sequence\n",
        "          self.getProbs(stem) # compute if not in wheels dict\n",
        "          n = self.wheels[stem]['count'] # return bigram count matching stem\n",
        "          c = self.c[bigram] # return bigram count, but some may not be present\n",
        "          if c == 0:\n",
        "            return float('-inf')\n",
        "          bigramProb += math.log(c / n, math.e)\n",
        "        return bigramProb\n",
        "        \n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        # Include START and END in probs, but exclude START from N\n",
        "        # get all bigrams and fetch their probs, do the math\n",
        "        if testCorpus is not None:\n",
        "          bigrams = [tuple(sent[i:i+2]) for sent in testCorpus for i in range(len(sent) - (2-1))]\n",
        "          N = len([word for sentence in testCorpus for word in sentence if word not in {START}]) # get list of words\n",
        "          logprobs = [self.logprobs[bigram] if bigram in self.c.keys() else float('-inf') for bigram in bigrams] # include START and END\n",
        "          return math.exp((-1/N) * sum(logprobs))\n",
        "        else:\n",
        "          return\n",
        "\n",
        "    def getAllProbs(self):\n",
        "        # iterate over all bigrams, compute stem count, compute bigram conditional prob\n",
        "        for bigram in self.bigrams:\n",
        "          count = self.c[bigram]\n",
        "          #if not self.wheels.get(bigram[0], False):\n",
        "            #self.getProbs(bigram[0])  \n",
        "          stemcount = self.c1[bigram[0]]\n",
        "          bigramcount = self.c[bigram]\n",
        "          self.logprobs[bigram] = math.log(bigramcount / stemcount, math.e)\n",
        "\n",
        "    def getVocab(self, corpus):\n",
        "        # get bigrams, vocab set, counts\n",
        "        self.bigrams = [tuple(sent[i:i+2]) for sent in corpus for i in range(len(sent) - (2-1))]\n",
        "        self.unigrams = [i for sent in corpus for i in sent]\n",
        "        self.vocab = set(self.unigrams)\n",
        "        self.c = Counter(self.bigrams)\n",
        "        self.c1 = Counter(self.unigrams)\n",
        "\n",
        "    def generateBigram(self, stem):\n",
        "        self.getProbs(stem)\n",
        "        roll = random.randrange(1, self.wheels[stem]['count'] + 1)\n",
        "        return self.wheels[stem][roll]\n",
        "\n",
        "    def getProbs(self, word):\n",
        "        # make wheel of fortune for bigram[0] starter\n",
        "        # every vocab word can be a starter except END\n",
        "        # for word in self.vocab:\n",
        "          if word != END and self.wheels.get(word, False) == False:\n",
        "            # return the bigrams and counts that match | also END returns no matches\n",
        "            # could also just use unigram counts\n",
        "            matches = {k: self.c[k] for k in self.c.keys() if k[0] == word}\n",
        "            # get total appearance count of the matching bigrams\n",
        "            N = sum(matches.values())\n",
        "            # index the wheel\n",
        "            self.wheels[word] = dict.fromkeys(['count', *range(1, N + 1)])\n",
        "            self.wheels[word]['count'] = N\n",
        "            # populate the wheel with end word\n",
        "            place = 1\n",
        "            for bigram in matches:\n",
        "              for x in range(1, matches[bigram] + 1):\n",
        "                self.wheels[word][place] = bigram[-1]\n",
        "                place += 1\n",
        "          else: \n",
        "            return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ3l-Ycn2ZRg",
        "outputId": "ba3ec2de-40e4-41c1-edac-4c86f0b9ebc4"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    testModel('bigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -inf \tModel log prob.: -inf \t PASSED \t ['<s>', 'SONIC', 'WAS', 'DIFFICULT', '.', '</s>']\n",
            "Correct log prob.: -10.3450917073 \tModel log prob.: -10.3450917073 \t PASSED \t ['<s>', 'BY', 'THE', 'LATE', 'CLASSIC', ',', 'A', 'NETWORK', 'OF', '<UNK>', '(', '<UNK>', ')', 'LINKED', 'VARIOUS', 'PARTS', 'OF', 'THE', 'CITY', ',', 'RUNNING', 'FOR', 'SEVERAL', 'KILOMETRES', 'THROUGH', 'ITS', 'URBAN', 'CORE', '.', '</s>']\n",
            "Correct log prob.: -9.2464794186 \tModel log prob.: -9.2464794186 \t PASSED \t ['<s>', 'FEW', 'PEOPLE', 'REALIZE', 'HOW', 'DIFFICULT', 'IT', 'WAS', 'TO', 'CREATE', 'SONIC', \"'S\", 'GRAPHICS', 'ENGINE', ',', 'WHICH', 'ALLOWED', 'FOR', 'THE', 'INCREDIBLE', 'RATE', 'OF', 'SPEED', 'THE', 'GAME', \"'S\", 'KNOWN', 'FOR', '.', '</s>']\n",
            "Correct log prob.: -inf \tModel log prob.: -inf \t PASSED \t ['<s>', 'CLASSIC', 'PARTS', 'OF', 'THE', 'GAME', 'ALLOWED', 'FOR', '<UNK>', 'INCREDIBLE', 'CITY', '.', '</s>']\n",
            "Correct log prob.: -inf \tModel log prob.: -inf \t PASSED \t ['<s>', 'FEW', '<UNK>', 'REALIZE', 'THE', 'DIFFICULT', 'NETWORK', ',', 'WHICH', 'LINKED', 'THE', 'GAME', 'TO', 'SONIC', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 1.4018400696 \tModel train perp.: 1.4018400696 \t PASSED\n",
            "Correct test perp.: inf \tModel test perp.: inf \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEZuC5uq2fkR",
        "outputId": "f54c5bf9-bb74-4e90-a56c-34290b2ce4f2"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('bigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -11.13855317607246 \tSentence: ['<s>', '483-70', 'CN2.5LT', 'HS', '</s>']\n",
            "Log Probability: -15.624939826070587 \tSentence: ['<s>', 'WB90', 'CN0.5L', 'STBLU', 'MIX159', 'DRK', 'YELLOW', '</s>']\n",
            "Log Probability: -12.3380427719357 \tSentence: ['<s>', 'LE3570S', 'CN1GA', 'CLEAR', '</s>']\n",
            "Log Probability: -14.3670964063428 \tSentence: ['<s>', 'WB1098', 'B1LT', 'HIGH', 'TEMP', 'ACTIVATOR', '(AC)', '</s>']\n",
            "Log Probability: -17.2116810053293 \tSentence: ['<s>', 'APP205', 'CN1GA', 'REDUCER', 'V', 'HI', 'TEMP', 'ACTIVATOR', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 9.92092254811232\n",
            "Testing Set: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0O1hdr92l9P"
      },
      "source": [
        "## Smoothed Bigram Model\n",
        "\n",
        "A <b>bigram</b> model with <b>absolute discounting</b> follows. The probability distribution of a word is given by $P_{AD}(w’|w)$.\n",
        "\n",
        "Smoothing involves a discounting factor $D$. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, $D$ is: \n",
        "\n",
        "$$D=\\frac{n_1}{n_1+2n_2}$$ \n",
        "\n",
        "For each word $w$, the number of bigram types $ww’$ is computed as follows: \n",
        "\n",
        "$$S(w)=|\\{w’|c(ww’)>0\\}|$$ \n",
        "\n",
        "where $c(ww’)$ is the frequency of $ww’$ in the training data.\n",
        "\n",
        "Finally, $P_{AD}(w’|w)$ is computed: \n",
        "\n",
        "$$P_{AD}(w’|w)=\\frac{\\max \\big (c(ww’)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(w’)\\bigg )$$ \n",
        "\n",
        "where $c(w)$ is the frequency of $w$ in the training data and $P_L(w’)$ is the Laplace-smoothed unigram probability of $w’$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-21O4unN23GX"
      },
      "source": [
        "import copy\n",
        "from functools import lru_cache\n",
        "\n",
        "class SmoothedBigramModelAD(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        self.bigrams = [] #flattened tape of bigrams\n",
        "        self.unigrams = [] #flattened tape of unigrams excluding START\n",
        "        self.vocab = {} #set of unigrams\n",
        "        self.bicab = {} #set of bigrams\n",
        "        self.c2 = dict() #count of bigrams\n",
        "        self.c1 = dict() #count of unigrams\n",
        "\n",
        "        self.wheels = defaultdict(dict)\n",
        "\n",
        "        self.D = float() #discount factor\n",
        "        self.N = int() #num words in CORPUS tape, excluding START\n",
        "\n",
        "        self.lenS = int()\n",
        "        self.Sw = defaultdict(set) #num bigram types per stem\n",
        "        self.logprobs = dict()\n",
        "        self.PL = dict() # smoothed unigram probabilities (of follower) for AD\n",
        "\n",
        "        self.getCounts(trainCorpus)\n",
        "\n",
        "    def getCounts(self, corpus):\n",
        "        self.bigrams = [tuple(sent[i:i+2]) for sent in corpus for i in range(len(sent) - (2-1))]\n",
        "        self.unigrams = [UNK] + [i for sent in corpus for i in sent] # initialize with at least one UNK\n",
        "        self.bicab = set(self.bigrams)\n",
        "        self.vocab = set(self.unigrams)\n",
        "        self.c2 = Counter(self.bigrams)\n",
        "        self.c1 = Counter(self.unigrams)\n",
        "        \n",
        "        n1 = len({i for i in self.c2.keys() if self.c2[i] == 1})\n",
        "        n2 = len({i for i in self.c2.keys() if self.c2[i] == 2})\n",
        "        self.D = n1 / (n1 + 2 * n2)\n",
        "\n",
        "        self.lenS = len(self.vocab) - 1\n",
        "        self.N = len([i for i in self.unigrams if i not in {START}]) # must omit START token\n",
        "\n",
        "        # Initialize branching index for smoothing function\n",
        "        for bigram in self.bicab:\n",
        "          self.Sw[bigram[0]].add(bigram)\n",
        "        self.Sw[UNK].add((UNK, UNK)) # insert at least one UNK bigram, might tend to generate this\n",
        "\n",
        "    def generateSentence(self):\n",
        "      ### Start with START, get next word, repeat till END\n",
        "      ### Only draw from training corpus\n",
        "        sent = [START]\n",
        "        newWord = ''\n",
        "        while newWord != END:\n",
        "          newWord = self.generateBigram(sent[-1])\n",
        "          sent.append(newWord)\n",
        "        return sent  \n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "      ### Lazy-eval smoothed probs\n",
        "        if not self.bicab: return float('inf')\n",
        "        sentbigrams = [tuple(sentence[i:i+2]) for i in range(len(sentence) - (2-1))]\n",
        "        return sum([self.logprobs[bigram] if self.logprobs.get(bigram) else self.getADProb(bigram) for bigram in sentbigrams])\n",
        "   \n",
        "    def corpusWrapper(func):\n",
        "      ### Alter N for calculating perplexity on new corpus\n",
        "        def wrapper(self, testCorpus):\n",
        "          dummy = self.N\n",
        "          self.N = len([i for sent in testCorpus for i in sent if i not in {START}]) # must omit START token\n",
        "          output = func(self, testCorpus)\n",
        "          self.N = dummy\n",
        "          return output\n",
        "        return wrapper\n",
        "\n",
        "    @corpusWrapper\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "      ### Lazy-eval smoothed probs\n",
        "        if testCorpus is not None:\n",
        "          return math.exp((-1/self.N) * sum([self.getSentenceLogProbability(sent) for sent in testCorpus]))\n",
        "        else:\n",
        "          return\n",
        "\n",
        "    @lru_cache(256)\n",
        "    def getADProb(self, bigram):\n",
        "      ### Get abs discounted probs for unseen/seen bigrams (not OOV unigrams) \n",
        "      ### N differs between test and train operations \n",
        "        if bigram[0] not in self.vocab: stem = UNK\n",
        "        else: stem = bigram[0] \n",
        "        if bigram[1] not in self.vocab: follower = UNK\n",
        "        else: follower = bigram[1] \n",
        "        if stem not in self.vocab: return float('-inf') # a test for OOV or null input\n",
        "        stemcount = self.getCw(stem)\n",
        "        if stemcount == 0: \n",
        "          print('no stem in vocab count')\n",
        "          return float('-inf') \n",
        "        # Count of unique followers in traincorpus, or # times applied discount\n",
        "        Sw = self.getSw(stem)\n",
        "        # Laplace smoothed unigram prob of follower as above\n",
        "        PL = self.getPL(follower)\n",
        "        # Maintain dict of bigram probs\n",
        "        self.logprobs[bigram] = \\\n",
        "          math.log( (max(0, self.getCw0w1(bigram) - self.D) + (self.D * Sw * PL)) / stemcount, math.e) # possibly zero if Sw, PL zero\n",
        "        return self.logprobs.get(bigram)\n",
        "\n",
        "    @lru_cache(256)\n",
        "    def getSw(self, stem):\n",
        "      return len(self.Sw.get(stem)) # if stem is UNK, returns the dummy training case of len 1\n",
        "\n",
        "    @lru_cache(256)\n",
        "    def getPL(self, follower):\n",
        "      if not self.PL.get(follower):\n",
        "        self.PL[follower] = (self.getCw(follower) + 1) / (self.N + self.lenS)\n",
        "      return self.PL.get(follower)\n",
        "\n",
        "    @lru_cache(256) \n",
        "    def getCw(self, unigram):\n",
        "      return self.c1.get(unigram, self.c1.get(UNK)) # returns UNK count of >= 1 if OOV\n",
        "\n",
        "    @lru_cache(256)\n",
        "    def getCw0w1(self, bigram):\n",
        "      return self.c2.get(bigram, 0)\n",
        "\n",
        "\n",
        "    def generateBigram(self, stem):\n",
        "      # support func for generateSentence\n",
        "        self.getProbs(stem)\n",
        "        roll = random.randrange(1, self.wheels[stem]['count'] + 1)\n",
        "        return self.wheels[stem][roll]\n",
        "\n",
        "    def getProbs(self, word):\n",
        "      ### support func for generateSentence\n",
        "      ### make wheel of fortune for bigram[0] starter\n",
        "      ### every vocab word can be a starter except END\n",
        "        if word != END and self.wheels.get(word, False) == False:\n",
        "          # return the bigrams and counts that match | also END returns no matches\n",
        "          # could also just use unigram counts\n",
        "          matches = {k: self.c2[k] for k in self.c2.keys() if k[0] == word}\n",
        "          # get total appearance count of the matching bigrams\n",
        "          N = sum(matches.values())\n",
        "          # index the wheel\n",
        "          self.wheels[word] = dict.fromkeys(['count', *range(1, N + 1)])\n",
        "          self.wheels[word]['count'] = N\n",
        "          # populate the wheel with end word\n",
        "          place = 1\n",
        "          for bigram in matches:\n",
        "            for x in range(1, matches[bigram] + 1):\n",
        "              self.wheels[word][place] = bigram[-1]\n",
        "              place += 1\n",
        "        else: \n",
        "          return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Bm8wKG2-0u",
        "outputId": "84fb883a-0b16-4537-bb6a-925933e52130"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    testModel('smoothed-bigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -16.1336514995 \tModel log prob.: -16.172553647 \t FAILED \t ['<s>', 'SONIC', 'WAS', 'DIFFICULT', '.', '</s>']\n",
            "Correct log prob.: -84.9068097328 \tModel log prob.: -85.0403277868 \t FAILED \t ['<s>', 'BY', 'THE', 'LATE', 'CLASSIC', ',', 'A', 'NETWORK', 'OF', '<UNK>', '(', '<UNK>', ')', 'LINKED', 'VARIOUS', 'PARTS', 'OF', 'THE', 'CITY', ',', 'RUNNING', 'FOR', 'SEVERAL', 'KILOMETRES', 'THROUGH', 'ITS', 'URBAN', 'CORE', '.', '</s>']\n",
            "Correct log prob.: -84.6431458887 \tModel log prob.: -84.7732458836 \t FAILED \t ['<s>', 'FEW', 'PEOPLE', 'REALIZE', 'HOW', 'DIFFICULT', 'IT', 'WAS', 'TO', 'CREATE', 'SONIC', \"'S\", 'GRAPHICS', 'ENGINE', ',', 'WHICH', 'ALLOWED', 'FOR', 'THE', 'INCREDIBLE', 'RATE', 'OF', 'SPEED', 'THE', 'GAME', \"'S\", 'KNOWN', 'FOR', '.', '</s>']\n",
            "Correct log prob.: -39.3603940053 \tModel log prob.: -39.1650030398 \t FAILED \t ['<s>', 'CLASSIC', 'PARTS', 'OF', 'THE', 'GAME', 'ALLOWED', 'FOR', '<UNK>', 'INCREDIBLE', 'CITY', '.', '</s>']\n",
            "Correct log prob.: -51.4605809045 \tModel log prob.: -51.2970430262 \t FAILED \t ['<s>', 'FEW', '<UNK>', 'REALIZE', 'THE', 'DIFFICULT', 'NETWORK', ',', 'WHICH', 'LINKED', 'THE', 'GAME', 'TO', 'SONIC', '.', '</s>']\n",
            "Test getSentenceProbability(...) failed on 5 sentences...\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 18.6021115212 \tModel train perp.: 18.6868531349 \t FAILED\n",
            "Correct test perp.: 28.8970586024 \tModel test perp.: 28.5154529925 \t FAILED\n",
            "Test getCorpusPerplexity(...) failed on the training corpus and the testing corpus...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKZj12UJ3AbM",
        "outputId": "25103d66-bd98-4bba-cca3-9c1203d0394a"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('smoothed-bigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -21.697686493523868 \tSentence: ['<s>', 'G2-4500S', 'CN1GA', 'METALUX', 'COARSE', 'ALUM', '</s>']\n",
            "Log Probability: -14.612159694755531 \tSentence: ['<s>', '29586000', 'CN5LT', 'THINNER', '</s>']\n",
            "Log Probability: -15.326072177939247 \tSentence: ['<s>', 'WB05', 'CN1L', 'PS', 'VHS', 'HARDENER', '</s>']\n",
            "Log Probability: -12.188504028224338 \tSentence: ['<s>', '2311S', 'TBXX', 'SANDING', 'PASTE', '</s>']\n",
            "Log Probability: -21.72001091201145 \tSentence: ['<s>', '7701S', 'CN1GA', 'STANDARD', 'ACTIVATOR', '(AC)', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 22.38044063709131\n",
            "Testing Set: 135.40496418312708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LdpXHSH0xi6"
      },
      "source": [
        "# Text Classification with Neural Networks\n",
        "We will use neural networks to classify product descriptions using the sample dataset. \n",
        "\n",
        "Some reference PyTorch tutorials:\n",
        "<ul>\n",
        "<li>https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "<li>https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "<li>https://github.com/yunjey/pytorch-tutorial\n",
        "</ul>\n",
        "\n",
        "Note: sparingly use \"GPU\" as the runtime type, as this will speed up the training of your models. You can find this by going to <TT>Runtime > Change Runtime Type</TT> and select \"GPU\" from the dropdown menu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkUZ2kIE1Rum",
        "outputId": "b2dde239-56fc-4ddc-83e2-dd6ecc475bfd"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import torchtext \n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if __name__=='__main__':\n",
        "    print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvDc-nuu1Y4t"
      },
      "source": [
        "# Step 2: Load Data\n",
        "First we load the dataset as `train_data` and `test_data` and do some basic tokenization.\n",
        "\n",
        "*   To access the list of textual tokens for the *i*th example, use `train_data[i][1]`\n",
        "*   To access the label for the *i*th example, use `train_data[i][0]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7ZJMQlO1h6x",
        "outputId": "280e21b1-44ec-4630-b1b1-24db3b48e500"
      },
      "source": [
        "import pandas \n",
        "\n",
        "def preprocess_NN(item):\n",
        "    '''\n",
        "    #Process each individual item at a time\n",
        "    '''\n",
        "    if not item or type(item) != str: return # filter blanks\n",
        "    #if len(item) < 2: return    \n",
        "    result = []\n",
        "    for x in item.split(' '):\n",
        "      # Trim any needless starting or ending chars from tokens\n",
        "        if not x or x.isspace(): continue # filter blanks\n",
        "        else:  \n",
        "          try:\n",
        "            remove_beg=True if x[0] in {'(', '\"', \"'\"} else False\n",
        "            remove_end=True if x[-1] in {'.', ',', ';', ':', '?', '!', '\"', \"'\", ')'} else False\n",
        "          except:\n",
        "            print(\"Error on item: \",item, ord(x))\n",
        "          if remove_beg and remove_end: result += [x[0], x[1:-1], x[-1]]\n",
        "          elif remove_beg: result += [x[0], x[1:]]\n",
        "          elif remove_end: result += [x[:-1], x[-1]]\n",
        "          else: result += [x]\n",
        "    return result\n",
        "\n",
        "\n",
        "def getDataset_NN():\n",
        "    \"\"\"\n",
        "    Returns 2 dataframes of Code, Name, PRODHA, dataset, where PRODHA is '?' in the test set\n",
        "    \"\"\"\n",
        "    df = []\n",
        "    path = '/content/drive/MyDrive/data/Test6_REF.csv'\n",
        "\n",
        "    filedata = pandas.read_csv(path, \n",
        "                         header = 0, \n",
        "                         names = ['Code', 'Name', 'PRODHA', 'dataset'] \n",
        "                         )\n",
        "\n",
        "    df.append(filedata[filedata['dataset'] == 'train']) #df[0]\n",
        "    df.append(filedata[filedata['dataset'] == 'test']) #df[1]\n",
        "    '''\n",
        "    try:\n",
        "      train_dataset = preprocess_NN(df[0].iteritems(['Name'])\n",
        "    except:\n",
        "      print(1)\n",
        "    try:\n",
        "      test_dataset = preprocess_NN(df[1]['Name'])\n",
        "    except:\n",
        "      print(1)\n",
        "    '''\n",
        "    return df[0], df[1]\n",
        "\n",
        "if __name__=='__main__':\n",
        "    train_data1, test_data1 = getDataset_NN()\n",
        "    train_data1 = [(x[0], preprocess_NN(x[1])) for x in train_data1.loc[:, ['PRODHA','Name']].itertuples(index=False) \\\n",
        "                   if x[1] is not None and x[0] is not None and type(x[1]) == str]\n",
        "    test_data1 = [(x[0], preprocess_NN(x[1])) for x in test_data1.loc[:, ['PRODHA','Name']].itertuples(index=False) \\\n",
        "                   if x[1] is not None and x[0] is not None and type(x[1]) == str]\n",
        "    #train_data, test_data = train_data[0:10000] + train_data[12500:12500+10000], train_data[10000:12500] + train_data[12500+10000:], \n",
        "\n",
        "    # print(train_data1[1:10])  \n",
        "    print('Num. Train Examples:', len(train_data1))\n",
        "    print('Num. Test Examples:', len(test_data1))\n",
        "\n",
        "    print(\"\\nSAMPLE TRAIN DATA:\")\n",
        "    for x in random.sample(train_data1, 5):\n",
        "        print('Sample text:', x[1])\n",
        "        print('Sample label:', x[0], '\\n')\n",
        "        #print('Sample index:', x[2], '\\n')\n",
        "    \n",
        "    print(\"\\nSAMPLE TEST DATA:\")\n",
        "    for x in random.sample(test_data1, 5):\n",
        "        print('Sample text:', x[1])\n",
        "        print('Sample label:', x[0], '\\n')\n",
        "        #print('Sample index:', x[2], '\\n')\n",
        "\n",
        "    #for x in train_data1:\n",
        "    #  print (x[0], x[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num. Train Examples: 1387\n",
            "Num. Test Examples: 2980\n",
            "\n",
            "SAMPLE TRAIN DATA:\n",
            "Sample text: ['442-21', 'CN1LT', 'NASONXL', 'BLUE']\n",
            "Sample label: A1A0015054J6U500A1 \n",
            "\n",
            "Sample text: ['LV984', 'CN1PT', 'METALUX', 'RED', 'OXIDE']\n",
            "Sample label: A1A0015054J6U500A1 \n",
            "\n",
            "Sample text: ['29185106', 'CN1L', 'PC', 'ANTI-SILI', 'ADDITIV', '8510']\n",
            "Sample label: A1A012501212WC00A1 \n",
            "\n",
            "Sample text: ['29105980', 'CN1LT', 'PC295', 'MB598', 'DIAM', 'GREEN']\n",
            "Sample label: A1A0015054J6GV00A1 \n",
            "\n",
            "Sample text: ['WB68', 'CN0.5LT', 'CROMAX', 'PRO', 'DARK', 'VIOLET']\n",
            "Sample label: A1A0015154J6HQ00A1 \n",
            "\n",
            "\n",
            "SAMPLE TEST DATA:\n",
            "Sample text: ['DX707', 'CN3.785LT', 'DUXONE', 'BLACK']\n",
            "Sample label: ? \n",
            "\n",
            "Sample text: ['CH8106', 'CN1PT', 'CHALLENGER', 'LV', 'CLARET', 'RED']\n",
            "Sample label: ? \n",
            "\n",
            "Sample text: ['422-50', 'CN1QT', 'NASON', 'GRAY', 'SEALER']\n",
            "Sample label: ? \n",
            "\n",
            "Sample text: ['EAW*', 'CNS4', 'IMRON', 'ELITE', 'SINGLE', 'STAGE']\n",
            "Sample label: ? \n",
            "\n",
            "Sample text: ['L1010', 'CN1LT', 'LMB', 'FINE', 'WHITE', 'MICA']\n",
            "Sample label: ? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncTzvoxWT03p"
      },
      "source": [
        "## Step 3: Create PyTorch Dataloader\n",
        "\n",
        "Here is the <b>dataset</b> class containing the tokenized data for NN models. \n",
        "\n",
        "*   <b>` build_dictionary(self)`:</b>  Creates the dictionaries `idx2word` and `word2idx`. Represent each word in the dataset with a unique index, and keep track of this in these dictionaries. Use the hyperparameter `threshold` to control which words appear in the dictionary: a training word’s frequency should be `>= threshold` to be included in the dictionary.\n",
        "\n",
        "* <b>`convert_text(self)`:</b> Converts each review in the dataset to a list of indices, given by `word2idx` dictionary. Store this in the `textual_ids` variable, and the function does not return anything. If a word is not present in the  `word2idx` dictionary, use the `<UNK>` token for that word. Be sure to append the `<END>` token to the end of each review.\n",
        "\n",
        "*   <b>` get_text(self, idx) `:</b> Return the review at `idx` in the dataset as an array of indices corresponding to the words in the review. If the length of the review is less than `max_len`, pad the review with the `<PAD>` character up to the length of `max_len`. If the length is greater than `max_len`, then it should only return the first `max_len` words. The return type should be `torch.LongTensor`.\n",
        "\n",
        "*   <b>`get_label(self, idx) `</b>: Return the value `1` if the label for `idx` in the dataset is `positive`, and should return `0` if it is `negative`. The return type should be `torch.LongTensor`.\n",
        "\n",
        "*  <b> ` __len__(self) `:</b> Return the total number of reviews in the dataset as an `int`.\n",
        "\n",
        "*   <b>` __getitem__(self, idx)`:</b> Return the (padded) text, and the label. The return type for both these items should be `torch.LongTensor`. Use the ` get_label(self, idx) ` and ` get_text(self, idx) ` functions here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqoj1Bt2TWIL"
      },
      "source": [
        "PAD = '<PAD>'\n",
        "END = '<END>'\n",
        "UNK = '<UNK>'\n",
        "from collections import Counter\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, examples, split, threshold, max_len, idx2word=None, word2idx=None, label2idx=None):\n",
        "\n",
        "        self.examples = examples\n",
        "        assert split in {'train', 'val', 'test'}\n",
        "        self.split = split\n",
        "        self.threshold = threshold\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Dictionaries\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.label2idx = label2idx\n",
        "        if split == 'train':\n",
        "          self.build_dictionary()\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        self.label_size = len(self.label2idx)\n",
        "      \n",
        "        # Convert text to indices\n",
        "        self.textual_ids = []\n",
        "        self.label_ids = []\n",
        "        self.convert_text()\n",
        "        self.convert_labels()\n",
        "    \n",
        "        # Convert back to text\n",
        "        self.idx2label = {v: k for k, v in self.label2idx.items()}\n",
        "\n",
        "    def build_dictionary(self): \n",
        "        '''\n",
        "        Builds the dictionaries idx2word, word2idx and a label 'dictionary' label2idx. \n",
        "        This is only called when split='train', as these\n",
        "        dictionaries are passed in to the __init__(...) function otherwise. Uses self.threshold\n",
        "        to control which words are assigned indices in the dictionaries.\n",
        "        Returns nothing.\n",
        "        '''\n",
        "        assert self.split == 'train'\n",
        "        \n",
        "        self.idx2word = {0:PAD, 1:END, 2: UNK}\n",
        "        self.word2idx = {PAD:0, END:1, UNK: 2}\n",
        "        self.label2idx = {UNK:0}\n",
        "        self.idx2label = {0:UNK}\n",
        "\n",
        "        # Count the frequencies of all words in the training data (self.examples)\n",
        "        # Assign idx (starting from 3) to all words having word_freq >= self.threshold\n",
        "        # Convert to uppercase\n",
        "        words = [word.upper() for (_, desc) in self.examples for word in desc if word != None]\n",
        "\n",
        "        rawcounts = Counter(words)\n",
        "        adjcounts = rawcounts - Counter(dict.fromkeys(rawcounts, self.threshold - 1)) #Subtract same keys with counts equal to threshold\n",
        "        +adjcounts # Removes all zero count keys, hence the -1 above\n",
        "\n",
        "        try:\n",
        "          test = set([label for label, desc in self.examples])\n",
        "        except:\n",
        "          print(test)\n",
        "        idx2wordadder = dict(enumerate(adjcounts.keys(), start=3))\n",
        "        word2idxadder = {value: key for key, value in idx2wordadder.items()}\n",
        "        label2idxadder = {label: i for i, label in enumerate(test, start=1)}\n",
        "        idx2labeladder = {v: k for k, v in label2idxadder.items()}\n",
        "        self.idx2word.update(idx2wordadder)\n",
        "        self.word2idx.update(word2idxadder)\n",
        "        self.label2idx.update(label2idxadder)\n",
        "        self.idx2label.update(idx2labeladder)\n",
        "        return\n",
        "    \n",
        "    def convert_text(self):\n",
        "        '''\n",
        "        Converts each product desc in the dataset (self.examples) to a list of indices, given by self.word2idx.\n",
        "        Store this in self.textual_ids; returns nothing.\n",
        "        '''\n",
        "        # list of lists; replaces a word with the <UNK> token if it does not exist in the word2idx dictionary.\n",
        "        self.textual_ids = [[self.word2idx[word] if self.word2idx.get(word) else self.word2idx[UNK] for word in \\\n",
        "                             (Word.upper() for Word in description)] \\\n",
        "                            for (_, description) in self.examples]\n",
        "\n",
        "        # Appends the <END> token to the end of each review.\n",
        "        for description in self.textual_ids:\n",
        "          description.append(self.word2idx[END])\n",
        "        return\n",
        "    \n",
        "    def convert_labels(self):\n",
        "        '''\n",
        "        Converts each product category code in the dataset (self.examples) to an index value, given by self.label2idx. \n",
        "        Stored in self.label_ids; returns nothing.\n",
        "        '''\n",
        "        self.label_ids = [self.label2idx[item[0]] if self.label2idx.get(item[0]) else self.label2idx[UNK] \\\n",
        "                          for item in self.examples] \\\n",
        "                          #for (label, alist) in item]\n",
        "        return\n",
        "    \n",
        "    def get_text(self, idx):\n",
        "        '''\n",
        "        Tokenizes product desc.\n",
        "        Returns the desc at idx as a long tensor (torch.LongTensor) of integers corresponding to the words in the desc.\n",
        "        May need to PAD.\n",
        "        '''\n",
        "        # REVIEW idx\n",
        "        a = self.textual_ids[idx]\n",
        "        b = [a[x] if x < len(a) else self.word2idx[PAD] for x in range(0, self.max_len)]\n",
        "        return torch.LongTensor(b)\n",
        "    \n",
        "    def get_label(self, idx):\n",
        "        '''\n",
        "        Returns the integer value of the label per idx.\n",
        "        type = torch.LongTensor.\n",
        "        '''\n",
        "        a = self.label_ids[idx]\n",
        "        return torch.tensor(a, dtype=torch.long)\n",
        "\n",
        "    def get_labeltext(self, idx):\n",
        "        '''\n",
        "        Returns original product desc text per idx.\n",
        "        type = string.\n",
        "        '''\n",
        "        b = self.idx2label[idx]\n",
        "        return b\n",
        "    \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns the number of examples (int value) in the dataset\n",
        "        '''\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Returns the product desc, and label of the example specified by idx.\n",
        "        '''\n",
        "        return self.get_text(idx), self.get_label(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZiFHYn2U2_G",
        "outputId": "23fd3167-c6c6-4a74-bfe9-acce94702883"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    # Sample item\n",
        "    Ds = TextDataset(train_data1, split='train', threshold=1, max_len=8)\n",
        "    print('Vocab size:', Ds.vocab_size)\n",
        "    print('Label set:', Ds.label_size)\n",
        "\n",
        "    text, label = Ds[random.randint(0, len(Ds))]\n",
        "    print('Example text:', text)\n",
        "    print('Example label:', label)\n",
        "\n",
        "    # For checking only. Ignored downstream.\n",
        "    Ds2 = TextDataset(test_data1, split='test', threshold=1, max_len=8, word2idx=Ds.word2idx, idx2word=Ds.idx2word, label2idx=Ds.label2idx)\n",
        "    print('Vocab size:', Ds2.vocab_size)\n",
        "    print('Label set:', Ds2.label_size)\n",
        "    \n",
        "    text, label = Ds2[random.randint(0, len(Ds2))]\n",
        "    print('Example text:', text)\n",
        "    print('Example label:', label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 2409\n",
            "Label set: 172\n",
            "Example text: tensor([2378, 1080,  676,  701,    1,    0,    0,    0])\n",
            "Example label: tensor(87)\n",
            "Vocab size: 2409\n",
            "Label set: 172\n",
            "Example text: tensor([   2,    2, 1060,    1,    0,    0,    0,    0])\n",
            "Example label: tensor(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzvoTZwmU5WJ"
      },
      "source": [
        "# Step 3: Train a Convolutional Neural Network (CNN)\n",
        "## Define the CNN Model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHyq-TgyVCn3"
      },
      "source": [
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, out_channels, filter_heights, stride, dropout, num_classes, pad_idx):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "        #   to represent the words in vocabulary. Make sure to use vocab_size, embed_size, and pad_idx here.\n",
        "\n",
        "        self.embeddinglayer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=pad_idx)\n",
        "\n",
        "        # Define multiple Convolution layers (nn.Conv2d) with filter (kernel) size [filter_height, embed_size] based on \n",
        "        #   different filter_heights.\n",
        "        # Input channels will be 1 and output channels will be out_channels (these many different filters will be trained \n",
        "        #   for each convolution layer)\n",
        "        # Note: even though conv layers are nn.Conv2d, we are doing a 1d convolution since we are only moving the filter \n",
        "        #   in one direction\n",
        "          # kernel_size - (height, width)\n",
        "\n",
        "        self.convlayers = nn.ModuleList(\n",
        "          [nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(i, embed_size), stride=stride) for i in filter_heights]\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Create a dropout layer (nn.Dropout) using dropout\n",
        "        \n",
        "        self.dropoutlayer = nn.Dropout(p=dropout, inplace=False)\n",
        "\n",
        "        # Define a linear layer (nn.Linear) that consists of num_classes units \n",
        "        #   and takes as input the concatenated output for all cnn layers (out_channels * num_of_cnn_layers units)\n",
        "        # length of filter_heights equals the number of layers, marc=out_channels*layers\n",
        "        \n",
        "        self.linearlayer = nn.Linear(in_features=out_channels*len(filter_heights), out_features=num_classes, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, texts):\n",
        "        \"\"\"\n",
        "        texts: LongTensor [batch_size, max_len]\n",
        "        \n",
        "        Returns output: Tensor [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        # Pass texts through embedding layer to convert from word ids to word embeddings\n",
        "        #   Resulting: shape: [batch_size, max_len, embed_size]\n",
        "        \n",
        "        x = self.embeddinglayer(texts)\n",
        "        #print('after embed [batch_size, max_len, embed_size]:', x.shape)\n",
        "\n",
        "        # Input to conv should have 1 channel.\n",
        "        #   Resulting shape: [batch_size, 1, MAX_LEN, embed_size]\n",
        "        \n",
        "        x = torch.unsqueeze(x, 1)\n",
        "        #print('after unsqueeze [batch_size, 1, MAX_LEN, embed_size]:', x.shape)\n",
        "        \n",
        "        # Pass these texts to each conv layer and compute their output as follows:\n",
        "        #   Output will have shape [batch_size, out_channels, *, 1] where * depends on filter_height and stride\n",
        "        #   Convert to shape [batch_size, out_channels, *] (see torch's squeeze() function)\n",
        "        #   Apply non-linearity on it \n",
        "        #   Take the max value across last dimension to have shape [batch_size, out_channels]\n",
        "        # Concatenate (torch.cat) outputs from all cnns [batch_size, (out_channels*num_of_cnn_layers)]\n",
        "        #\n",
        "\n",
        "        results = [None] * len(self.convlayers)\n",
        "        for i, conv in enumerate(self.convlayers):\n",
        "          results[i] = conv(x)\n",
        "          #print('after conv [batch_size, out_channels, *, 1]:', results[i].shape)\n",
        "          results[i] = torch.squeeze(results[i], dim=3)\n",
        "          #print('after squeezed:', results[i].shape)\n",
        "          results[i] = self.relu(results[i])\n",
        "          #print('after relu:', results[i].shape)\n",
        "          results[i] = torch.max(results[i], dim=2)[0]\n",
        "          #print('after max:', results[i].shape)\n",
        "          #results[i] = torch.squeeze(results[i])\n",
        "          #print('after squeezed:', results[i].shape)\n",
        "        x = torch.cat(results, dim=1)\n",
        "        #print('after cat [batch_size, (out_channels*num_of_cnn_layers)]:', x.shape)\n",
        "\n",
        "        #   Since each cnn is of different filter_height, it will look at different number of words at a time\n",
        "        #     So, a filter_height of 3 means cnn looks at 3 words (3-grams) at a time and tries to extract some information from it\n",
        "        #   Each cnn will learn out_channels number of features from the words it sees at a time\n",
        "        #   Then applied a non-linearity and took the max value for all channels\n",
        "        #    Essentially trying to find important n-grams from the entire text\n",
        "        # Everything happens on a batch simultaneously hence that additional batch_size as the first dimension\n",
        "\n",
        "        # Apply dropout\n",
        "        \n",
        "        x = self.dropoutlayer(x)\n",
        "        #print('after dropout:', x.shape)\n",
        "\n",
        "        # Pass output through the linear layer and return its output \n",
        "        #   Resulting shape: [batch_size, num_classes]\n",
        "        # (((W - K + 2P)/S) + 1)\n",
        "\n",
        "        x = self.linearlayer(x)\n",
        "        #print('after linear [batch_size, num_classes]:', x.shape)\n",
        "\n",
        "        ##### NOTE: Do not apply a sigmoid or softmax to the final output - done in training method!\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCxm3avbVcum"
      },
      "source": [
        "## Train CNN Model\n",
        "\n",
        "First, we initialize the train and test <b>dataloaders</b>. A dataloader is responsible for providing batches of data to the model. We first instantiate datasets for the train and test data, and that we use the training vocabulary for both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvgwsOugVd9q"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    THRESHOLD = 1\n",
        "    MAX_LEN = 8\n",
        "    BATCH_SIZE = 32 \n",
        "\n",
        "    train_Ds = TextDataset(train_data1, 'train', THRESHOLD, MAX_LEN)\n",
        "    train_loader = torch.utils.data.DataLoader(train_Ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "    test_Ds = TextDataset(test_data1, 'test', THRESHOLD, MAX_LEN, train_Ds.idx2word, train_Ds.word2idx, train_Ds.label2idx)\n",
        "    test_loader = torch.utils.data.DataLoader(test_Ds, batch_size=1, shuffle=False, num_workers=1, drop_last=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYz9XAs_Whja"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_model(model, num_epochs, data_loader, optimizer, criterion):\n",
        "    print('Training Model...')\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for texts, labels in data_loader:\n",
        "            texts = texts.to(device) # shape: [batch_size, MAX_LEN]\n",
        "            labels = labels.to(device) # shape: [batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(texts)\n",
        "            acc = accuracy(output, labels)\n",
        "            \n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        print('[TRAIN]\\t Epoch: {:2d}\\t Loss: {:.4f}\\t Train Accuracy: {:.2f}%'.format(epoch+1, epoch_loss/len(data_loader), 100*epoch_acc/len(data_loader)))\n",
        "    print('Model Trained!\\n')\n",
        "    \n",
        "    torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKLhBkhDWvlC"
      },
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Count number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch\n",
        "    output: Tensor [batch_size, n_classes]\n",
        "    labels: LongTensor [batch_size]\n",
        "    \"\"\"\n",
        "    preds = output.argmax(dim=1) # find predicted class\n",
        "    correct = (preds == labels).sum().float() # convert into float for division \n",
        "    acc = correct / len(labels)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hd2lu-yW3D_"
      },
      "source": [
        "Now we instantiate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smQvgpGZW8QG",
        "outputId": "1b9e8151-8c80-4a5f-d4b6-1f3d7a040d2a"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    cnn_model = CNN(vocab_size = train_Ds.vocab_size, \n",
        "                embed_size = 128, \n",
        "                out_channels = 64, \n",
        "                filter_heights = [2, 3, 4], \n",
        "                stride = 1, \n",
        "                dropout = 0.5, \n",
        "                num_classes = train_Ds.label_size, \n",
        "                pad_idx = train_Ds.word2idx[PAD])\n",
        "\n",
        "    # Put model on the device (cuda or cpu)\n",
        "    cnn_model = cnn_model.to(device)\n",
        "    \n",
        "    print('The model has {:,d} trainable parameters'.format(count_parameters(cnn_model)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 415,468 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtQC1c3eXLLh"
      },
      "source": [
        "Next, we create the **criterion**, which is our loss function: it is a measure of how well the model matches the empirical distribution of the data. We use cross-entropy loss (https://en.wikipedia.org/wiki/Cross_entropy).\n",
        "\n",
        "We also define the **optimizer**, which performs gradient descent. We use the Adam optimizer (https://arxiv.org/pdf/1412.6980.pdf), which has been shown to work well on these types of models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1sSCijOXMdc"
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    LEARNING_RATE = 5e-4 \n",
        "\n",
        "    # Define the loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LdHL98CXRLb"
      },
      "source": [
        "Finally, we can train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "e0331439800040c1999eb61de14a43c4",
            "3dc8349bdbf24f97b75b1147c74436ce",
            "546f686e6ea349abbe2b2fa6a7950850",
            "7e3b771741cf48fea8c390feea1cd91a",
            "53d61dd314f247dab1bc6d5ac2e556b6",
            "a8d91d21447149ecaaeb54b522268372",
            "ee9c7446178e4943afb506b5a33d6134",
            "9d0cc7fd619244628a657d0fe22e6dac",
            "da8e20bcadfd48758fa678113b5787f8",
            "60a3a2796cee4c66a79ad10117871c39",
            "5f5aa9681fae411a954f9ea7f6965203"
          ]
        },
        "id": "-h41v96zXQ7h",
        "outputId": "50bc16dd-7474-4135-f226-29d4c5638a26"
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    N_EPOCHS = 25 \n",
        "    \n",
        "    # train model for N_EPOCHS epochs\n",
        "    train_model(cnn_model, N_EPOCHS, train_loader, optimizer, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0331439800040c1999eb61de14a43c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]\t Epoch:  1\t Loss: 4.4988\t Train Accuracy: 9.45%\n",
            "[TRAIN]\t Epoch:  2\t Loss: 3.5988\t Train Accuracy: 23.55%\n",
            "[TRAIN]\t Epoch:  3\t Loss: 3.1791\t Train Accuracy: 32.78%\n",
            "[TRAIN]\t Epoch:  4\t Loss: 2.8150\t Train Accuracy: 41.50%\n",
            "[TRAIN]\t Epoch:  5\t Loss: 2.5158\t Train Accuracy: 47.53%\n",
            "[TRAIN]\t Epoch:  6\t Loss: 2.2722\t Train Accuracy: 52.11%\n",
            "[TRAIN]\t Epoch:  7\t Loss: 2.0158\t Train Accuracy: 56.18%\n",
            "[TRAIN]\t Epoch:  8\t Loss: 1.8040\t Train Accuracy: 62.06%\n",
            "[TRAIN]\t Epoch:  9\t Loss: 1.6122\t Train Accuracy: 65.70%\n",
            "[TRAIN]\t Epoch: 10\t Loss: 1.4007\t Train Accuracy: 70.42%\n",
            "[TRAIN]\t Epoch: 11\t Loss: 1.2390\t Train Accuracy: 74.42%\n",
            "[TRAIN]\t Epoch: 12\t Loss: 1.1189\t Train Accuracy: 76.96%\n",
            "[TRAIN]\t Epoch: 13\t Loss: 1.0044\t Train Accuracy: 78.56%\n",
            "[TRAIN]\t Epoch: 14\t Loss: 0.8859\t Train Accuracy: 81.40%\n",
            "[TRAIN]\t Epoch: 15\t Loss: 0.7953\t Train Accuracy: 84.88%\n",
            "[TRAIN]\t Epoch: 16\t Loss: 0.7078\t Train Accuracy: 85.83%\n",
            "[TRAIN]\t Epoch: 17\t Loss: 0.6416\t Train Accuracy: 88.01%\n",
            "[TRAIN]\t Epoch: 18\t Loss: 0.5545\t Train Accuracy: 89.90%\n",
            "[TRAIN]\t Epoch: 19\t Loss: 0.4892\t Train Accuracy: 90.99%\n",
            "[TRAIN]\t Epoch: 20\t Loss: 0.4299\t Train Accuracy: 93.02%\n",
            "[TRAIN]\t Epoch: 21\t Loss: 0.3801\t Train Accuracy: 93.39%\n",
            "[TRAIN]\t Epoch: 22\t Loss: 0.3371\t Train Accuracy: 95.20%\n",
            "[TRAIN]\t Epoch: 23\t Loss: 0.3183\t Train Accuracy: 94.62%\n",
            "[TRAIN]\t Epoch: 24\t Loss: 0.2820\t Train Accuracy: 96.00%\n",
            "[TRAIN]\t Epoch: 25\t Loss: 0.2704\t Train Accuracy: 95.64%\n",
            "Model Trained!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9tb5GaXp-s"
      },
      "source": [
        "## Evaluate CNN Model\n",
        "\n",
        "Now that the model is trained for text classification, it is time to evaluate it with the following function. Only compare across the same datasets!\n",
        "NOTE: test dataset may be the predict dataset, and accuracy will report zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvg5nbF7X17f"
      },
      "source": [
        "import random\n",
        "\n",
        "def evaluate(model, data_loader, criterion, dataset):\n",
        "    print('Evaluating performance on the test dataset...')\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    all_predictions = []\n",
        "    all_textpredictions = []\n",
        "    print(\"\\nSOME PREDICTIONS FROM THE MODEL:\")\n",
        "    for texts, labels in tqdm(data_loader):\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        output = model(texts)\n",
        "        acc = accuracy(output, labels)\n",
        "        pred = output.argmax(dim=1)\n",
        "        \n",
        "        all_predictions.append(pred)\n",
        "        a = dataset.get_labeltext(pred.item()) # string\n",
        "        all_textpredictions.append(a)\n",
        "        \n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "        if random.random() < 0.01:\n",
        "            print(\"Input: \"+' '.join([data_loader.dataset.idx2word[idx] for idx in texts[0].tolist() if idx not in {data_loader.dataset.word2idx[PAD], data_loader.dataset.word2idx[END]}]))\n",
        "            print(\"Prediction:\", pred.item(), '\\tCorrect Output:', labels.item(), '\\n')\n",
        "\n",
        "    full_acc = 100*epoch_acc/len(data_loader)\n",
        "    full_loss = epoch_loss/len(data_loader)\n",
        "    print('[TEST]\\t Loss: {:.4f}\\t Accuracy: {:.2f}%'.format(full_loss, full_acc))\n",
        "    predictions = torch.cat(all_predictions)\n",
        "   \n",
        "\n",
        "    return predictions, full_acc, full_loss, all_textpredictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "67cd05a2a73d4e5db7886d945f6d14e2",
            "7a526e91db314f1e9a32ef2cd8250652",
            "f0dd1f3cf5e54a76a837b7b72ce71874",
            "8935360068f942cebd95baa49ccc3fd7",
            "d279a37a88a04ff887fbff14069553ef",
            "c8db82d1fe5d4970ae0df43a08a0f990",
            "190cd3b5e19a429d9dedab69f2a9715e",
            "fb0a46fa3ebf453c948e713bd92c79ba",
            "589679cffc6a4368b08c76a6bca4a9e8",
            "e08bee32eaf549459ae237fb886f417d",
            "80f2e2ab0f4d4eecbe627a63bed85a62"
          ]
        },
        "id": "dlRZaJJRX5zF",
        "outputId": "341cd09e-39a9-4e05-daff-be67ffddb75c"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    _ , _ , _ , output = evaluate(cnn_model, test_loader, criterion, test_Ds) # Compute test data accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating performance on the test dataset...\n",
            "\n",
            "SOME PREDICTIONS FROM THE MODEL:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67cd05a2a73d4e5db7886d945f6d14e2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2980 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <UNK> CN0.5L HYDROCLR BC <UNK>\n",
            "Prediction: 124 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> PASTE <UNK>\n",
            "Prediction: 87 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1QT IMRON 6600 CT BASECOAT\n",
            "Prediction: 11 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN0.5LT STANDOX ADDITIVE MAROON\n",
            "Prediction: 124 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> BO1PT SATIN PEARL\n",
            "Prediction: 40 \tCorrect Output: 0 \n",
            "\n",
            "Input: 2001 CN1QT LMB 4:1 CLEAR FAST ACT\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1HP ULTRA <UNK> CLEAR ACTIVATOR\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> EAXX CROMAX EX WBC <UNK> PT LBL\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1QT IMRON <UNK> PRO BC MIX\n",
            "Prediction: 4 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> <UNK>\n",
            "Prediction: 87 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> <UNK>\n",
            "Prediction: 87 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN01 <UNK>\n",
            "Prediction: 87 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1GA BLACK PEARL FUL-THANE\n",
            "Prediction: 153 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> SCXX GLOSS WHITE AEROSOL\n",
            "Prediction: 158 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1PT NASONXL RED SHADE ORANGE\n",
            "Prediction: 126 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> <UNK> <UNK> LBL\n",
            "Prediction: 152 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> EAXX METAL <UNK> PANEL <UNK>\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> EAXX POLY SPRAY <UNK> ( <UNK> )\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> EAXX 10 <UNK> <UNK> <UNK> <UNK>\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "Prediction: 164 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1LT NASONXL PEARL GREEN\n",
            "Prediction: 35 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN0.5LT NASONXL PRL RADIANT GOLD\n",
            "Prediction: 126 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1LT 5.5 ETCH PRMR ACTIVATR\n",
            "Prediction: 10 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1QT TRANSPARENT RED\n",
            "Prediction: 11 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1GA <UNK> PRIMER - WHITE\n",
            "Prediction: 33 \tCorrect Output: 0 \n",
            "\n",
            "Input: CH2054 CN1GA CHALLENGER 2K BLACK SEALER\n",
            "Prediction: 161 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1LT CHALLENGER SEA GREEN PEARL\n",
            "Prediction: 12 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> <UNK> <UNK>\n",
            "Prediction: 87 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1QT DUXONE LOW VOC MEDIUM BLUE\n",
            "Prediction: 152 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN1LT LMB MEDIUM BLUE MICA\n",
            "Prediction: 35 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> CN0.5LT LUMABASE TRANS MAGENTA BS\n",
            "Prediction: 126 \tCorrect Output: 0 \n",
            "\n",
            "Input: PT185 CN1QT RED OXIDE TINT\n",
            "Prediction: 11 \tCorrect Output: 0 \n",
            "\n",
            "Input: <UNK> <UNK> <UNK> <UNK> CLEAR\n",
            "Prediction: 29 \tCorrect Output: 0 \n",
            "\n",
            "[TEST]\t Loss: 11.2785\t Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL8DqH_SYa7m"
      },
      "source": [
        "# Step 4: Train a Recurrent Neural Network (RNN)\n",
        "We now build a text classification model that is based on **recurrences** or auto-regression between hidden states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H1I1dDPYavb"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, bidirectional, dropout, num_classes, pad_idx):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "        #   to represent the words in the vocabulary. Make sure to use vocab_size, embed_size, and pad_idx here.\n",
        "        self.embedlayer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=pad_idx)\n",
        "\n",
        "        # Create a recurrent network (use nn.GRU, not nn.LSTM) with batch_first = True\n",
        "        # hidden_size, num_layers, dropout, and bidirectional used here\n",
        "        self.rnnlayer = nn.GRU(input_size=embed_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout, bidirectional = bidirectional)\n",
        "        \n",
        "        # Create a dropout layer (nn.Dropout) using dropout\n",
        "        self.dropoutlayer = nn.Dropout(p=dropout, inplace=False)\n",
        "\n",
        "        # Define a linear layer (nn.Linear) that consists of num_classes units \n",
        "        #   and takes as input the output of the last timestep. In the bidirectional case, can concatenate\n",
        "        #   the output of the last timestep of the forward direction with the output of the last timestep of the backward direction.\n",
        "        #GRU output = (N, L, D, Hout) where L is sequence length, Hout is hidden_size, D is 2 if bidirectional\n",
        "        self.linearlayer = nn.Linear(in_features=self.hidden_size if bidirectional==False else self.hidden_size*2, out_features=num_classes, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, texts):\n",
        "        \"\"\"texts: LongTensor [batch_size, MAX_LEN]\n",
        "        Returns output: Tensor [batch_size, num_classes]\"\"\"\n",
        "\n",
        "        # Pass texts through the embedding layer to convert from word ids to word embeddings\n",
        "        #   Resulting: shape: [batch_size, max_len, embed_size]\n",
        "        x = self.embedlayer(texts)\n",
        "        #print('after embed [batch_size, max_len, embed_size]:', x.shape)\n",
        "\n",
        "        # Pass the result through the recurrent network\n",
        "        #   See PyTorch documentation for resulting shape for nn.GRU\n",
        "        # hidden > (D * num_layers, N, H_{out})(D∗num_layers,N,Hout)\n",
        "        x, state = self.rnnlayer(x)\n",
        "        #print('after rnn (batch_size, *, {1 or 2} * hidden_size):', x.shape)\n",
        "        #print('after rnn hidden states [D * num_layers, batch_size, hidden_size]:', state.shape)\n",
        "        \n",
        "        # Concatenate the outputs of the last timestep for each direction (see torch.cat(...))\n",
        "        #   This depends on whether or not the model is bidirectional\n",
        "        #   Resulting shape: [batch_size, num_dirs*hidden_size]\n",
        "        #state = torch.transpose(state, 0, 1)\n",
        "        #print('after transpose [batch_size, * , *]:', state.shape)\n",
        "\n",
        "        #state.view(:, self.num_layers, -1, self.hidden_size)\n",
        "        #state = state[:,-2]\n",
        "        #print('after reshape [batch_size, num_dirs*hidden_size]:', state.shape)\n",
        "\n",
        "\n",
        "        # OR, just take last element (timestamp) of the rnn Output\n",
        "        x = x[:,-1]\n",
        "        #print('sliced:', x.shape)\n",
        "        #print('both reshaped? ', torch.equal(state, x))\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropoutlayer(x)\n",
        "        #print('after dropout (no change) :', x.shape)\n",
        "        \n",
        "        # Pass the output through the linear layer and return its output \n",
        "        #   Resulting shape: [batch_size, num_classes]\n",
        "        x = self.linearlayer(x)\n",
        "        \n",
        "        #print('after linear [batch_size, num_classes]:', x.shape)\n",
        "\n",
        "        ##### NOTE: Do not apply a sigmoid or softmax to the final output - done in training method!\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJmZZC9EY0yG"
      },
      "source": [
        "## Train RNN Model\n",
        "First, we initialize the train and test dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z__PIAw-Y3mP"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    THRESHOLD = 1 # count to surpass to make it into vocab\n",
        "    MAX_LEN = 8 # 'window' of description tokens\n",
        "    BATCH_SIZE = 32 \n",
        "\n",
        "    train_Ds = TextDataset(train_data1, 'train', THRESHOLD, MAX_LEN)\n",
        "    train_loader = torch.utils.data.DataLoader(train_Ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "    test_Ds = TextDataset(test_data1, 'test', THRESHOLD, MAX_LEN, train_Ds.idx2word, train_Ds.word2idx, train_Ds.label2idx)\n",
        "    test_loader = torch.utils.data.DataLoader(test_Ds, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rucz6WxZI10",
        "outputId": "9d4f6f3e-1950-4fa9-f946-f06d5701263c"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    rnn_model = RNN(vocab_size = train_Ds.vocab_size,\n",
        "                embed_size = 64, \n",
        "                hidden_size = 64, \n",
        "                num_layers = 2,\n",
        "                bidirectional = True,\n",
        "                dropout = 0.5,\n",
        "                num_classes = train_Ds.label_size,\n",
        "                pad_idx = train_Ds.word2idx[PAD]) \n",
        "\n",
        "    # Put model on device\n",
        "    rnn_model = rnn_model.to(device)\n",
        "\n",
        "    print('The model has {:,d} trainable parameters'.format(count_parameters(rnn_model)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 571,093 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izmAk-rIZfvm"
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    LEARNING_RATE = 5e-4 # tweakable\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991,
          "referenced_widgets": [
            "7e15f0da078b4d93b5e91d81112bb889",
            "b10f685b691d434e9dc1c9eaad2bb94e",
            "775e42be26e844a8ab410f74e6dae516",
            "81cb29a0a2924c8da93554f3072615cc",
            "04c098bd10314c67994e71c5eaa25cbe",
            "1acd56421efc43d0bd265a2b2fcf1281",
            "d1958b70c0f54f1b8b9c58adcb497711",
            "d68aced551444f86a93eca1c7ab739d6",
            "3761b08185bd42769f0ae9559cf32193",
            "377c566a6b11472f9c777666b0c45207",
            "a6ce2df6c5cf4651a39458a5edfdc0c8"
          ]
        },
        "id": "UjmamTC2ZmNp",
        "outputId": "1637c39b-370b-4a7e-9dc7-7131f6d8ec5a"
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    N_EPOCHS = 50 # Tweakable\n",
        "    \n",
        "    # train model for N_EPOCHS epochs\n",
        "    train_model(rnn_model, N_EPOCHS, train_loader, optimizer, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e15f0da078b4d93b5e91d81112bb889",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]\t Epoch:  1\t Loss: 2.3760\t Train Accuracy: 47.36%\n",
            "[TRAIN]\t Epoch:  2\t Loss: 2.2171\t Train Accuracy: 50.62%\n",
            "[TRAIN]\t Epoch:  3\t Loss: 2.0612\t Train Accuracy: 54.29%\n",
            "[TRAIN]\t Epoch:  4\t Loss: 1.9362\t Train Accuracy: 56.19%\n",
            "[TRAIN]\t Epoch:  5\t Loss: 1.8445\t Train Accuracy: 58.25%\n",
            "[TRAIN]\t Epoch:  6\t Loss: 1.7273\t Train Accuracy: 59.54%\n",
            "[TRAIN]\t Epoch:  7\t Loss: 1.6342\t Train Accuracy: 61.10%\n",
            "[TRAIN]\t Epoch:  8\t Loss: 1.5511\t Train Accuracy: 63.14%\n",
            "[TRAIN]\t Epoch:  9\t Loss: 1.4620\t Train Accuracy: 65.23%\n",
            "[TRAIN]\t Epoch: 10\t Loss: 1.4004\t Train Accuracy: 66.67%\n",
            "[TRAIN]\t Epoch: 11\t Loss: 1.3252\t Train Accuracy: 67.95%\n",
            "[TRAIN]\t Epoch: 12\t Loss: 1.2576\t Train Accuracy: 69.50%\n",
            "[TRAIN]\t Epoch: 13\t Loss: 1.2123\t Train Accuracy: 70.71%\n",
            "[TRAIN]\t Epoch: 14\t Loss: 1.1451\t Train Accuracy: 72.06%\n",
            "[TRAIN]\t Epoch: 15\t Loss: 1.0886\t Train Accuracy: 73.09%\n",
            "[TRAIN]\t Epoch: 16\t Loss: 1.0500\t Train Accuracy: 74.31%\n",
            "[TRAIN]\t Epoch: 17\t Loss: 1.0008\t Train Accuracy: 75.43%\n",
            "[TRAIN]\t Epoch: 18\t Loss: 0.9454\t Train Accuracy: 76.59%\n",
            "[TRAIN]\t Epoch: 19\t Loss: 0.9076\t Train Accuracy: 77.40%\n",
            "[TRAIN]\t Epoch: 20\t Loss: 0.8526\t Train Accuracy: 78.54%\n",
            "[TRAIN]\t Epoch: 21\t Loss: 0.8188\t Train Accuracy: 79.58%\n",
            "[TRAIN]\t Epoch: 22\t Loss: 0.7950\t Train Accuracy: 80.48%\n",
            "[TRAIN]\t Epoch: 23\t Loss: 0.7460\t Train Accuracy: 81.19%\n",
            "[TRAIN]\t Epoch: 24\t Loss: 0.7190\t Train Accuracy: 81.64%\n",
            "[TRAIN]\t Epoch: 25\t Loss: 0.6977\t Train Accuracy: 82.80%\n",
            "[TRAIN]\t Epoch: 26\t Loss: 0.6658\t Train Accuracy: 83.40%\n",
            "[TRAIN]\t Epoch: 27\t Loss: 0.6254\t Train Accuracy: 84.08%\n",
            "[TRAIN]\t Epoch: 28\t Loss: 0.6103\t Train Accuracy: 84.36%\n",
            "[TRAIN]\t Epoch: 29\t Loss: 0.5851\t Train Accuracy: 84.86%\n",
            "[TRAIN]\t Epoch: 30\t Loss: 0.5554\t Train Accuracy: 86.43%\n",
            "[TRAIN]\t Epoch: 31\t Loss: 0.5489\t Train Accuracy: 86.25%\n",
            "[TRAIN]\t Epoch: 32\t Loss: 0.5223\t Train Accuracy: 86.30%\n",
            "[TRAIN]\t Epoch: 33\t Loss: 0.5014\t Train Accuracy: 87.29%\n",
            "[TRAIN]\t Epoch: 34\t Loss: 0.4834\t Train Accuracy: 87.99%\n",
            "[TRAIN]\t Epoch: 35\t Loss: 0.4459\t Train Accuracy: 89.02%\n",
            "[TRAIN]\t Epoch: 36\t Loss: 0.4462\t Train Accuracy: 88.57%\n",
            "[TRAIN]\t Epoch: 37\t Loss: 0.4138\t Train Accuracy: 90.06%\n",
            "[TRAIN]\t Epoch: 38\t Loss: 0.4012\t Train Accuracy: 89.90%\n",
            "[TRAIN]\t Epoch: 39\t Loss: 0.3886\t Train Accuracy: 90.49%\n",
            "[TRAIN]\t Epoch: 40\t Loss: 0.3782\t Train Accuracy: 90.98%\n",
            "[TRAIN]\t Epoch: 41\t Loss: 0.3638\t Train Accuracy: 90.85%\n",
            "[TRAIN]\t Epoch: 42\t Loss: 0.3578\t Train Accuracy: 91.11%\n",
            "[TRAIN]\t Epoch: 43\t Loss: 0.3349\t Train Accuracy: 91.34%\n",
            "[TRAIN]\t Epoch: 44\t Loss: 0.3362\t Train Accuracy: 91.86%\n",
            "[TRAIN]\t Epoch: 45\t Loss: 0.3183\t Train Accuracy: 92.20%\n",
            "[TRAIN]\t Epoch: 46\t Loss: 0.3049\t Train Accuracy: 92.53%\n",
            "[TRAIN]\t Epoch: 47\t Loss: 0.2992\t Train Accuracy: 92.51%\n",
            "[TRAIN]\t Epoch: 48\t Loss: 0.2871\t Train Accuracy: 93.04%\n",
            "[TRAIN]\t Epoch: 49\t Loss: 0.2890\t Train Accuracy: 92.89%\n",
            "[TRAIN]\t Epoch: 50\t Loss: 0.2713\t Train Accuracy: 93.34%\n",
            "Model Trained!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKSmqjndaGmd"
      },
      "source": [
        "## Evaluate RNN Model\n",
        "\n",
        "Now we can evaluate the RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "5aae0b2d82ec4211b91894d18c58b25f",
            "c3117e2b4f294e98b16e5d5dd01c7251",
            "78c52b3bc27d492caa6cb21bca3dfa85",
            "ad3f7e0e7e604a4ea9f2944a17f6c1b8",
            "d191a2dc13af4092b86778c144baceee",
            "45fb4512c2b44636a0b9e3cf2e07b2c5",
            "876c136ab99340b2a1cb32686a89536d",
            "753f700ffd704821b9a068991a115a15",
            "9185e7d380304abc9b8c03537195414a",
            "d5eb3594745e447ca4d3ac9e3995cfda",
            "a4c40cd8feec4fc988a89aa4d0683bf1"
          ]
        },
        "id": "N_PS2XO5aJiL",
        "outputId": "48847548-9e81-42b4-9303-14403ef45880"
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    evaluate(rnn_model, test_loader, criterion) # Compute test data accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating performance on the test dataset...\n",
            "\n",
            "SOME PREDICTIONS FROM THE MODEL:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aae0b2d82ec4211b91894d18c58b25f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1284 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <UNK> <UNK> LT RAL1028 HB -COMB\n",
            "Prediction: 84 \tCorrect Output: 242 \n",
            "\n",
            "Input: ALESTA EE | RAL5001 GREEN BLUE SOLID SEMIGLOSS\n",
            "Prediction: 140 \tCorrect Output: 140 \n",
            "\n",
            "Input: ALESTA EP | RAL1021 YELLOW SOLID GLOSS\n",
            "Prediction: 21 \tCorrect Output: 21 \n",
            "\n",
            "Input: <UNK> TINT RED VIOLET\n",
            "Prediction: 99 \tCorrect Output: 99 \n",
            "\n",
            "Input: CROMAX | G2-7600SX CHROMACLEAR 4LT\n",
            "Prediction: 136 \tCorrect Output: 136 \n",
            "\n",
            "Input: 4210-01 N4LT NASON PU SURFACER EX\n",
            "Prediction: 82 \tCorrect Output: 82 \n",
            "\n",
            "Input: <UNK> N20LT TSA DAZZLING WHITE VE-T\n",
            "Prediction: 30 \tCorrect Output: 235 \n",
            "\n",
            "[TEST]\t Loss: 1.9357\t Accuracy: 65.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDml-qdaXBK"
      },
      "source": [
        "This piece caches the trained models to Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxE20yNOabqV",
        "outputId": "d8df1f64-60f1-434f-ec69-d1d3cc3477b6"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "        cnn_model is None\n",
        "        cnn_exists = True\n",
        "    except:\n",
        "        cnn_exists = False\n",
        "\n",
        "    try:\n",
        "        rnn_model is None\n",
        "        rnn_exists = True\n",
        "    except:\n",
        "        rnn_exists = False\n",
        "\n",
        "    if cnn_exists:\n",
        "        print(\"Saving CNN model....\") \n",
        "        torch.save(cnn_model, \"drive/My Drive/cnn.pt\")\n",
        "    if rnn_exists:\n",
        "        print(\"Saving RNN model....\") \n",
        "        torch.save(rnn_model, \"drive/My Drive/rnn.pt\")\n",
        "    print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Saving CNN model....\n",
            "Saving RNN model....\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save with pickle\n"
      ],
      "metadata": {
        "id": "iRBeTrruSq3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(cnn_model.state_dict(), MODELPATH + MODELFILENAME)"
      ],
      "metadata": {
        "id": "WXZYhUTCSwMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "EXPORTER\n",
        "'''\n",
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "with open('CNNoutput.csv', 'w') as myfile:\n",
        "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
        "    wr.writerow(output)\n",
        "\n",
        "files.download('CNNoutput.csv')"
      ],
      "metadata": {
        "id": "yc2OQ2SeWAW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}